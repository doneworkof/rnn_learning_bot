{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340e5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from extracting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505c106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdda9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "vector_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ec1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mini_wordlist.bf', 'rb') as f:\n",
    "    wordlist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b445be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_dict = dict([\n",
    "    (wordlist['word'][i], torch.tensor(wordlist['vector'][i])) for i in range(len(wordlist))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2df0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_filter(words):\n",
    "    return [w for w in words if w in wordlist_dict]\n",
    "\n",
    "def prepare(line):\n",
    "    extracted = extract(line)\n",
    "    seq = exist_filter(extracted) + ['<end>']\n",
    "    if len(seq) >= seq_len:\n",
    "        seq = seq[len(seq) - seq_len:]\n",
    "    else:\n",
    "        seq += ['<empty>'] * (seq_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def prep_to_matrix(prep):\n",
    "    return vectorize(prep)\n",
    "\n",
    "def word2vec(word):\n",
    "    idx = np.where(wordlist['word'] == word)[0][0]\n",
    "    return wordlist['vector'][idx]\n",
    "\n",
    "def line_to_matrix(line):\n",
    "    extracted = extract(line)\n",
    "    seq = exist_filter(extracted) + ['<end>']\n",
    "    if len(seq) >= seq_len:\n",
    "        seq = seq[len(seq) - seq_len:]\n",
    "    else:\n",
    "        seq += ['<empty>'] * (seq_len - len(seq))\n",
    "    return [\n",
    "        word2vec(w) for w in seq\n",
    "    ]\n",
    "    \n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=0, eps=1e-5)\n",
    "\n",
    "def get_most_similar(vec):\n",
    "    mp = wordlist['word'].apply(lambda x: cosine_similarity(vec, wordlist_dict[x]))\n",
    "    idx = np.argmax(mp)\n",
    "    return wordlist.iloc[idx]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b1d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(torch.nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))\n",
    "        y = self.module(x_reshape)\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "31394bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.last_output = None\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.max_train_size = 50\n",
    "        self.rnn1 = torch.nn.LSTM(vector_len, vector_len, 1, batch_first=True)\n",
    "        self.rnn2 = torch.nn.LSTM(vector_len, vector_len, 1, batch_first=True)\n",
    "        self.rnn3 = torch.nn.LSTM(vector_len, vector_len, 1, batch_first=True)\n",
    "        self.rnn4 = torch.nn.LSTM(vector_len, vector_len, 1, batch_first=True)\n",
    "        self.rnn5 = torch.nn.LSTM(vector_len, vector_len, 1, batch_first=True)\n",
    "        self.td_linear = TimeDistributed(\n",
    "            torch.nn.Linear(vector_len, vector_len), batch_first=True\n",
    "        )\n",
    "        self.last_hidden = None\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def extract_words(self, x):\n",
    "        words = []\n",
    "        for vec in x[0]:\n",
    "            word = get_most_similar(vec)\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            elif word == '<empty>' or (words and words[-1] == word):\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return words\n",
    "    \n",
    "    def append_train_data(self, x, y):\n",
    "        x = x.reshape(seq_len, -1).tolist()\n",
    "        y = y.reshape(seq_len, -1).tolist()\n",
    "        self.x_train.append(x)\n",
    "        self.y_train.append(y)\n",
    "        if len(self.x_train) > self.max_train_size:\n",
    "            self.x_train = self.x_train[1:]\n",
    "            self.y_train = self.y_train[1:]\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.x_train) == 0:\n",
    "            return\n",
    "        x_tr = torch.tensor(self.x_train)\n",
    "        y_tr = torch.tensor(self.y_train)\n",
    "        for ep in range(20):\n",
    "            self.optimizer.zero_grad()\n",
    "            pred = self.forward(x_tr, raw=True)\n",
    "            loss = self.criterion(pred, y_tr)\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def forward(self, x, raw=False):\n",
    "        if not raw:\n",
    "            if type(x) != str:\n",
    "                print('Error!')\n",
    "                return\n",
    "            x = torch.tensor(line_to_matrix(x)).reshape(1, seq_len, -1)\n",
    "            if self.last_output is not None:\n",
    "                self.append_train_data(self.last_output, x)\n",
    "        \n",
    "        if self.last_hidden and not raw:\n",
    "            h0, c0 = self.last_hidden\n",
    "        else:\n",
    "            h0 = torch.zeros((1, x.shape[0], vector_len))\n",
    "            c0 = torch.clone(h0)\n",
    "        x, hidden = self.rnn1(x.float(), (h0, c0))\n",
    "        x, hidden = self.rnn2(x, hidden)\n",
    "        x, hidden = self.rnn3(x, hidden)\n",
    "        x, hidden = self.rnn4(x, hidden)\n",
    "        x, hidden = self.rnn5(x, hidden)\n",
    "        x = self.td_linear(x)\n",
    "        \n",
    "        if raw:\n",
    "            return x\n",
    "        else:\n",
    "            self.last_hidden = hidden\n",
    "        \n",
    "        self.update()\n",
    "        words = self.extract_words(x)\n",
    "        words_raw = words + ['<end>'] + ['<empty>'] * (9 - len(words))\n",
    "        self.last_output = torch.tensor([\n",
    "            wordlist_dict[w].tolist() for w in words_raw\n",
    "        ])\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def fit(self, x, y, epochs=200, batch_size=100):\n",
    "        for ep in range(epochs):\n",
    "            for i in range(len(x) // batch_size + 1):\n",
    "                x_tr = x[i * batch_size:(i + 1) * batch_size]\n",
    "                y_tr = y[i * batch_size:(i + 1) * batch_size]\n",
    "                if len(x_tr) == 0:\n",
    "                        break\n",
    "                self.optimizer.zero_grad()\n",
    "                pred = self.forward(x_tr, raw=True)\n",
    "                loss = self.criterion(pred, y_tr)\n",
    "                print('Epoch', ep, 'Loss', loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogs_builder(dialogs):\n",
    "    x = []\n",
    "    y = []\n",
    "    print('TARGET:', len(dialogs))\n",
    "    for k, dialog in enumerate(dialogs):\n",
    "        try:\n",
    "            if (k + 1) % 100 == 0:\n",
    "                print('PROGRESS:', k + 1)\n",
    "            \n",
    "            mat = [\n",
    "                line_to_matrix(line) for line in dialog\n",
    "            ]\n",
    "\n",
    "            for i in range(len(mat) - 1):\n",
    "                x.append(mat[i])\n",
    "                y.append(mat[i + 1])\n",
    "        except Exception as ex:\n",
    "            print('ERROR:', ex)\n",
    "            continue\n",
    "            \n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a27a2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b839194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop():\n",
    "    while inp := input('> '):\n",
    "        print(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b14c778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples(count):\n",
    "    movie_dialogs = extract_movie_dialogs(0, count)\n",
    "    daily_dialogs = extract_daily_dialogs(0, count)\n",
    "    dialogs = movie_dialogs + daily_dialogs\n",
    "    return dialogs_builder(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
